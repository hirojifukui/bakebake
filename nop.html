<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8" />
  <title>のっぺらぼうカメラ（修正版）</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    html, body { margin: 0; padding: 0; overflow: hidden; }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: auto;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>

  <script type="module">
    import * as tf from 'https://cdn.skypack.dev/@tensorflow/tfjs';
    import * as faceLandmarksDetection from 'https://cdn.skypack.dev/@tensorflow-models/face-landmarks-detection';

    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: "user" },
        audio: false
      });
      video.srcObject = stream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => {
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          resolve();
        };
      });
    }

    async function runApp() {
      await setupCamera();

      const model = await faceLandmarksDetection.load(
        faceLandmarksDetection.SupportedPackages.mediapipeFacemesh,
        { maxFaces: 1 }
      );

      async function detect() {
        const faces = await model.estimateFaces({ input: video });
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        if (faces.length > 0) {
          const keypoints = faces[0].keypoints;
          const nose = keypoints[1];
          ctx.beginPath();
          ctx.arc(nose.x, nose.y, 10, 0, Math.PI * 2);
          ctx.fillStyle = "red";
          ctx.fill();
        }

        requestAnimationFrame(detect);
      }

      detect();
    }

    runApp();
  </script>
</body>
</html>
